{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "2.7.15"
    },
    "colab": {
      "name": "practicum-neurale-netwerken.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "OtlEse-Gfsoc"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4HOkhNyAfsnz"
      },
      "outputs": [],
      "source": [
        "# Implementatie van een tweelagig neuraal netwerk voor classificatie\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bk3Pla6Hfsn2"
      },
      "outputs": [],
      "source": [
        "Implementeer de sigmoïde:\n",
        "\n",
        "$\\sigma(z) = 1/(1+\\exp(-z))$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\" Z: een tensor met een willekeurige vorm\n",
        "    \n",
        "        returns: een tensor met dezelfde vorm van Z waarin de logistische functie \n",
        "                 werd toegepast op elk element van Z\n",
        "    \"\"\"\n",
        "    ### BEGIN CODE HIER (1 regel)\n",
        "    ### EINDE CODE HIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST CEL: niet wijzigen\n",
        "np.random.seed(1)\n",
        "assert sigmoid(0) == 0.5, \"Verkeerd in nul\"\n",
        "assert np.linalg.norm(sigmoid(np.zeros(2)) - np.array([0.5,0.5])) < 10**-6, \"Verkeerd in nul nul\"\n",
        "Z = np.random.randn(2,3)\n",
        "S = sigmoid(Z)\n",
        "assert Z.shape == S.shape, \"Verkeerde vorm\"\n",
        "Sinv = np.log(S/(1-S))\n",
        "assert np.linalg.norm(Z - Sinv) < 10**-6, \"Verkeerd in random punt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KEUKNWpNfsn6"
      },
      "outputs": [],
      "source": [
        "Bereken de uitvoer van het netwerk, alsook de tussenliggende activaties.\n",
        "We nemen aan dat alle lagen de sigmoïde activatiefunctie gebruiken.\n",
        "\n",
        "Elke laag doet het volgende:\n",
        "\n",
        "$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$\n",
        "en\n",
        "$A^{[l]} = \\sigma(Z^{[l]})$ voor $l \\in \\{1,2\\}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def forward_propagation(X, W1, W2, b1, b2):\n",
        "    \"\"\" Berekent het resultaat van een neuraal netwerk m.b.v. voorwaartse propagatie.\n",
        "    \n",
        "        X  : invoer van de vorm (n0, m)\n",
        "        W1 : eerste gewichten matrix van de vorm (n1, n0)\n",
        "        W2 : tweede gewichten matrix van de vorm (n2, n1)\n",
        "        b1 : bias vector van de eerste laag van de vorm (n1, 1)\n",
        "        b2 : bias vector van de tweede laag van de vorm (n2, 1)\n",
        "    \"\"\"\n",
        "    ### BEGIN CODE HIER (4 regels)\n",
        "\n",
        "    ### EINDE CODE HIER\n",
        "    return (A1, A2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST CEL: niet wijzigen\n",
        "W1 = np.array([[0.15,0.2],[0.25, 0.3]])\n",
        "b1 = np.array([0.35,0.35]).reshape(-1,1)\n",
        "W2 = np.array([[0.4,0.45],[0.5, 0.55]])\n",
        "b2 = np.array([0.6,0.6]).reshape(-1,1)\n",
        "X = np.array([0.05,0.1]).reshape(-1,1)\n",
        "A1, A2 = forward_propagation(X, W1, W2, b1, b2)\n",
        "expected1 = np.array([0.59326999, 0.59688438]).reshape(-1, 1)\n",
        "expected2 = np.array([0.75136507, 0.77292847]).reshape(-1,1)\n",
        "assert np.linalg.norm(A1 - expected1) < 10**-6, \"Verkeerde 1e laag voor 1 voorbeeld\"\n",
        "assert np.linalg.norm(A2 - expected2) < 10**-6, \"Verkeerde 2e laag voor 1 voorbeeld\"\n",
        "X = np.hstack([X, X, X])\n",
        "expected1 = np.hstack([expected1, expected1, expected1])\n",
        "expected2 = np.hstack([expected2, expected2, expected2])\n",
        "A1, A2 = forward_propagation(X, W1, W2, b1, b2)\n",
        "assert np.linalg.norm(A1 - expected1) < 10**-6, \"Verkeerde 1e laag voor 3 voorbeelden\"\n",
        "assert np.linalg.norm(A2 - expected2) < 10**-6, \"Verkeerde 2e laag voor 3 voorbeelden\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FsYwb20tfsoA"
      },
      "outputs": [],
      "source": [
        "We schrijven een functie om de kost te berekenen.\n",
        "\n",
        "$J = \n",
        "-\\frac{1}{m}\\sum_{i=1}^m \\sum_{k=1}^K\n",
        "y^{(i)}_k \\ln\\bigl(h(x^{(i)})_k\\bigr) + (1-y^{(i)}_k)\n",
        "\\ln\\bigl(1-h(x^{(i)})_k\\bigr).$\n",
        "\n",
        "In de invoer geeft <code>Yhat</code> de waarde van de hypothese aan.  Het is gemakkelijk \n",
        "om een een numpy array te bepalen met\n",
        "\n",
        "$ y^{(i)}_k \\ln\\bigl(h(x^{(i)})_k\\bigr) + (1-y^{(i)}_k)\n",
        "\\ln\\bigl(1-h(x^{(i)})_k\\bigr)$ \n",
        "op de plaats $(k, i)$. \n",
        "\n",
        "Vervolgens sommeer je van deze matrix \n",
        "de kolommen (en bekom je een array van de vorm $(1,m)$). Tenslotte bepaal je het gemiddelde\n",
        "van de elementen in deze array en vermenigvuldig je met $-1$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cost(Yhat, Y):\n",
        "    assert Yhat.shape == Y.shape\n",
        "    ### BEGIN CODE HIER (1 of 2 regels)\n",
        "    ### EINDE CODE HIER\n",
        "    cost = np.squeeze(cost)  ## Zorg ervoor dat de uitvoer als getal wordt teruggegeven\n",
        "    return cost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST Cel: Niet wijzigen\n",
        "X = np.array([0.05,0.1]).reshape(-1,1)\n",
        "Y = np.array([0,1]).reshape(-1, 1)\n",
        "Yhat = np.array([0.7514, 0.7729]).reshape(-1,1)\n",
        "c = cost(Yhat, Y)\n",
        "assert c.shape == (), \"Kost is geen scalair\"\n",
        "assert abs(c-1.6495157) < 10**-6, \"Verkeerde kost\"\n",
        "Yhat  = np.hstack([Yhat, Yhat, Yhat])\n",
        "Y = np.hstack([Y, Y, Y])\n",
        "c = cost(Yhat, Y)\n",
        "assert c.shape == (), \"Kost is geen scalair, meerdere voorbeelden\"\n",
        "assert abs(c-1.6495157) < 10**-6, \"Verkeerde kost, meerdere voorbeelden\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j71F8NOKfsoG"
      },
      "outputs": [],
      "source": [
        "We implementeren het algoritme voor achterwaartse propagatie waarbij de \n",
        "aanpassingsmatrices en aanpassingsvectoren worden bepaald.\n",
        "In de cursus staat hoe dit te doen voor één enkel voorbeeld:\n",
        "\\begin{equation}\\label{eq:backprop}\n",
        "\\left\\{\n",
        "\\begin{aligned}\n",
        "\\mathrm{d}z^{[2]} & \\gets a^{[2]} - y \\\\\n",
        "\\mathrm{d}W^{[2]} & \\gets \\mathrm{d}{z^{[2]}} {a^{[1]}}^T \\\\\n",
        "\\mathrm{d}b^{[2]} & \\gets \\mathrm{d}z^{[2]} \\\\\n",
        "\\mathrm{d}z^{[1]} & \\gets {W^{[2]}}^T  \\mathrm{d}z^{[2]} \\odot a^{[1]}\\odot(1-a^{[1]}) \\\\\n",
        "\\mathrm{d}W^{[1]} & \\gets \\mathrm{d}z^{[1]} a^{[0]^T} \\\\\n",
        "\\mathrm{d}b^{[1]} & \\gets \\mathrm{d}z^{[1]} \\\\\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "Het is echter mogelijk om dit ook te gaan vectoriseren en de berekeningen uit te voeren voor\n",
        "een aantal voorbeelden tegelijkertijd.  We krijgen:\n",
        "\\begin{equation}\n",
        "\\left\\{\n",
        "\\begin{aligned}\n",
        "\\mathrm{d}Z^{[2]} & \\gets A^{[2]} - Y                                                & (n^{[2]}, m)\\\\\n",
        "\\mathrm{d}W^{[2]} & \\gets \\frac{1}{m}\\mathrm{d}{Z^{[2]}} {A^{[1]}}^T                 & (n^{[2]}, n^{[1]})\\\\\n",
        "\\mathrm{d}b^{[2]} & \\gets \\frac{1}{m}\\mathrm{d}Z^{[2]} \\mathbb 1_{m\\times1}                              & (n^{[2]}, 1)\\\\\n",
        "\\mathrm{d}Z^{[1]} & \\gets {W^{[2]}}^T  \\mathrm{d}Z^{[2]} \\odot A^{[1]}\\odot(1-A^{[1]})  & (n^{[1]}, m)\\\\\n",
        "\\mathrm{d}W^{[1]} & \\gets \\frac{1}{m}\\mathrm{d}Z^{[1]} A^{[0]^T}               & (n^{[1]}, n^{[0]}) \\\\\n",
        "\\mathrm{d}b^{[1]} & \\gets \\frac{1}{m}\\mathrm{d}Z^{[1]}  \\mathbb 1_{m\\times1}    &    (n^{[1]}, 1)\\\\\n",
        "\\end{aligned}\n",
        "\\right.\n",
        "\\end{equation}\n",
        "\n",
        "Merk op dat de matrixvermenigvuldiging met $\\mathrm{d}Z \\mathbb 1_{m\\times 1}$ neerkomt op het nemen van de som van elke \n",
        "rij van $\\mathrm{d}Z$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def backward_propagation(X, Y, W1, W2, b1, b2, A1, A2):\n",
        "    \"\"\" Bepaal de aanpassingsmatrices en aanpassingsvectoren.\n",
        "    \n",
        "        X : invoer voor het neuraal netwerk van de vorm (n0, m)\n",
        "        Y : gewenste uitvoer van de vorm (n2, m)\n",
        "        W1 : eerste gewichten matrix van de vorm (n1, n0)\n",
        "        W2 : tweede gewichten matrix van de vorm (n2, n1)\n",
        "        b1 : eerste bias vector van de vorm (n1, 1)\n",
        "        b2 : tweede bias vector van de vorm (n2, 1)\n",
        "        \n",
        "        returns: de aanpassingmatrices en aanpassingsvectoren: dW1, dW2, db1, db2\n",
        "    \"\"\"\n",
        "    \n",
        "    (n, m) = X.shape\n",
        "    ### BEGIN CODE HIER (6 regels)\n",
        "   \n",
        "    ### EINDE CODE HIER\n",
        "    return dW1, dW2, db1, db2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "### TEST CEL: niet wijzigen\n",
        "X = np.array([0.05,0.1]).reshape(-1,1)\n",
        "Y = np.array([0,1]).reshape(-1, 1)\n",
        "W1 = np.array([[0.15,0.2],[0.25, 0.3]])\n",
        "b1 = np.array([0.35,0.35]).reshape(-1,1)\n",
        "W2 = np.array([[0.4,0.45],[0.5, 0.55]])\n",
        "b2 = np.array([0.6,0.6]).reshape(-1,1)\n",
        "A1, A2 = forward_propagation(X, W1, W2, b1, b2)\n",
        "dW1, dW2, db1, db2 = backward_propagation(X, Y, W1, W2, b1, b2, A1, A2)\n",
        "assert W1.shape == dW1.shape, \"Verkeerde vorm W1\"\n",
        "assert W2.shape == dW2.shape, \"Verkeerde vorm W2\"\n",
        "assert b1.shape == db1.shape, \"Verkeerde vorm b1\"\n",
        "assert b2.shape == db2.shape, \"Verkeerde vorm b2\"\n",
        "dW2_exp = np.array([[0.4458, 0.4485],[-0.1347, -0.1356]])\n",
        "db2_exp = np.array([0.7514, -0.2271]).reshape(-1,1)\n",
        "dW1_exp = np.array([[0.002256, 0.004512],[0.002565, 0.005130]])\n",
        "db1_exp = np.array([0.04512, 0.05130]).reshape(-1, 1)\n",
        "assert np.linalg.norm(dW2 - dW2_exp) < 10**-4, \"Verkeerde dW2\"\n",
        "assert np.linalg.norm(dW1 - dW1_exp) < 10**-6, \"Verkeerde dW1\"\n",
        "assert np.linalg.norm(db2 - db2_exp) < 10**-4, \"Verkeerde db2\"\n",
        "assert np.linalg.norm(db1 - db1_exp) < 10**-5, \"Verkeerde db1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iWcQiXUFfsoM"
      },
      "outputs": [],
      "source": [
        "De onderstaande twee functies berekenen samen de gradiënt van de kostfunctie op een alternatieve maar veel tragere manier.\n",
        "Elke paramter wordt lichtjes geperturbeerd en het effect hiervan op de waarde van de kostfunctie wordt bekeken.\n",
        "Op die manier kan de gradiënt numeriek worden bepaald.\n",
        "\n",
        "Het doel van deze trage (en eenvoudigere) implementatie is om te verifiëren dat de functie <code>backward_propagation</code> correct werkt.\n",
        "\n",
        "Je hoeft hier zelf niets te implementeren maar je zou wel moeten snappen wat de code doet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_gradient_slow(params, episolon, X, Y, W1, W2, b1, b2, epsilon=10**-6):\n",
        "    dp = np.zeros_like(params)\n",
        "    (rows, cols) = params.shape\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            backup = params[row, col]\n",
        "            params[row, col] += epsilon\n",
        "            _, Yhat_plus = forward_propagation(X, W1, W2, b1, b2)\n",
        "            cost_plus = cost(Yhat_plus, Y)\n",
        "            params[row, col] -= 2*epsilon\n",
        "            _, Yhat_min = forward_propagation(X, W1, W2, b1, b2)\n",
        "            cost_min = cost(Yhat_min, Y)\n",
        "            dp[row, col] = (cost_plus - cost_min)/(2*epsilon)\n",
        "            params[row, col] = backup\n",
        "    return dp\n",
        "\n",
        "## Bereken gradient op de trage manier\n",
        "def gradient_check(X, Y, W1, W2, b1, b2, epsilon = 10**-6):    \n",
        "    dW1 = compute_gradient_slow(W1, epsilon, X, Y, W1, W2, b1, b2)\n",
        "    dW2 = compute_gradient_slow(W2, epsilon, X, Y, W1, W2, b1, b2)\n",
        "    db1 = compute_gradient_slow(b1, epsilon, X, Y, W1, W2, b1, b2)\n",
        "    db2 = compute_gradient_slow(b2, epsilon, X, Y, W1, W2, b1, b2)\n",
        "    return dW1, dW2, db1, db2    \n",
        "\n",
        "\n",
        "## Kleine check voor wat random waarden\n",
        "np.random.seed(1)\n",
        "(n0, n1, n2) = (5, 10, 7)\n",
        "m = 100\n",
        "X = np.random.randn(n0, m)\n",
        "Y = np.random.randn(n2, m)\n",
        "W1 = np.random.randn(n1, n0)\n",
        "b1 = np.random.randn(n1,1)\n",
        "W2 = np.random.randn(n2, n1)\n",
        "b2 = np.random.randn(n2,1)\n",
        "\n",
        "A1, A2 = forward_propagation(X, W1, W2, b1, b2)\n",
        "dW1, dW2, db1, db2 = backward_propagation(X, Y, W1, W2, b1, b2, A1, A2)\n",
        "dW1_check, dW2_check, db1_check, db2_check = gradient_check(X, Y, W1, W2, b1, b2)\n",
        "\n",
        "assert np.linalg.norm(dW1 - dW1_check) < 10**-6, \"Foute gradient voor W1\"\n",
        "assert np.linalg.norm(dW2 - dW2_check) < 10**-6, \"Foute gradient voor W2\"\n",
        "assert np.linalg.norm(db1 - db1_check) < 10**-6, \"Foute gradient voor b1\"\n",
        "assert np.linalg.norm(db2 - db2_check) < 10**-6, \"Foute gradient voor b2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etNkM2p9fsoP"
      },
      "outputs": [],
      "source": [
        "Implementeer de code om de gewichtenmatrices en biasvectoren te initialiseren.\n",
        "De gewichtenmatrices initialiseer je met kleine random waarden, \n",
        "gebruik bv. <code>np.random.randn(...)*0.01</code>. De biasvectoren mogen \n",
        "geïntialiseerd worden met allemaal nullen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize(n0, n1, n2):\n",
        "    \"\"\" Bepaal random gewichtenmatrices en biasvectoren van de juiste vorm.\n",
        "    \n",
        "        n0 : aantal neuronen in de invoerlaag\n",
        "        n1 : aantal neuronen in de verborgen laag\n",
        "        n2 : aantal neuronen in de uitvoerlaag\n",
        "        \n",
        "        Returns: W1, W2, b1, b2\n",
        "    \"\"\"\n",
        "    ### BEGIN CODE HIER (4 regels)\n",
        "  \n",
        "    ### EINDE CODE HIER\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST Cel: Niet wijzigen\n",
        "(n0, n1, n2) = (4,7,5)\n",
        "W1, W2, b1, b2 = initialize(n0, n1, n2)\n",
        "assert W1.shape == (n1, n0), \"W1 heeft verkeerde vorm\"\n",
        "assert W2.shape == (n2, n1), \"W2 heeft verkeerde vorm\"\n",
        "assert b1.shape == (n1, 1), \"b1 heeft verkeerde vorm\"\n",
        "assert b2.shape == (n2, 1), \"b2 heeft verkeerde vorm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a0bejxcLf94J"
      },
      "outputs": [],
      "source": [
        "We schrijven nu de code om één enkele update uit te voeren, aannemend dat we de \n",
        "aanpassingsmatrices en -vectoren reeds hebben berekend.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def update(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha):\n",
        "    \"\"\" Voer één enkele update uit.\n",
        "    \n",
        "        W1 : eerste gewichten matrix van de vorm (n1, n0)\n",
        "        W2 : tweede gewichten matrix van de vorm (n2, n1)\n",
        "        b1 : eerste bias vector van de vorm (n1, 1)\n",
        "        b2 : tweede bias vector van de vorm (n2, 1)\n",
        "        dW1 : aanpassingsmatrix van de vorm (n1, n0)\n",
        "        dW2 : aanpassingsmatrix van de vorm (n2, n1)\n",
        "        db1 : aanpassingsvector van de vorm (n1, 1)\n",
        "        db2 : aanpassingsvector van de vorm (n2, 1)\n",
        "        alpha : de stapgrootte\n",
        "        \n",
        "        Returns: aangepaste versies van W1, W2, b1, b2\n",
        "    \"\"\"\n",
        "    ### BEGIN CODE HIER (4 regels)\n",
        "   \n",
        "    ### EINDE CODE HIER\n",
        "    return W1, W2, b1, b2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST Cel: Niet wijzigen\n",
        "(n0, n1, n2) = (4,7,5)\n",
        "np.random.seed(1)\n",
        "W1, W2, b1, b2 = initialize(n0, n1, n2)\n",
        "dW1, dW2, _, _ =initialize(n0, n1, n2)\n",
        "db1 = np.random.randn(n1, 1)\n",
        "db2 = np.random.randn(n2, 2)\n",
        "alpha = 0.05\n",
        "x, y, z, u = update(W1, W2, b1, b2, dW1, dW2, db1, db2, alpha)\n",
        "W1bis, W2bis, b1bis, b2bis = update(x, y, z, u, dW1, dW2, db1, db2, -alpha)\n",
        "assert np.linalg.norm(W1 - W1bis) < 10**-6, \"W1 verkeerd\"\n",
        "assert np.linalg.norm(W2 - W2bis) < 10**-6, \"W2 verkeerd\"\n",
        "assert np.linalg.norm(b1 - b1bis) < 10**-6, \"b1 verkeerd\"\n",
        "assert np.linalg.norm(b2 - b2bis) < 10**-6, \"b2 verkeerd\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RcMtILeofsoZ"
      },
      "outputs": [],
      "source": [
        "Nu implementeren we het eigenlijke optimalisatieproces.\n",
        "\n",
        "We delen de trainingsdata op in een minibatches. Per epoch overlopen we alle minibatches.\n",
        "Voor elke minibatch roepen we <code>forward_propagation</code> aan op de activaties te berekenen.\n",
        "Vervolgens roepen we <code>backward_propagation</code> en <code>update</code> aan.\n",
        "\n",
        "Om te verifiëren of het trainingsproces correct werkt houden we ook de kost per epoch bij. Wanneer\n",
        "het trainingsproces correct werkt moet deze kosthistoriek een dalende trend vertonen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(X, Y, W1, W2, b1, b2, num_epochs = 1000, mini_batch_size = 1, alpha = 0.05):\n",
        "    \"\"\"  Traint een neuraal netwerk en retourneert de getrainde gewichtenmatrices en biasvectoren.\n",
        "    \n",
        "        X  : Invoer voor het neuraal netwerk van de vorm (n0, m)\n",
        "        Y  : Gewenste uitvoer van de vorm (n2, m)\n",
        "        W1 : startwaarde eerste gewichtenmatrix, van de vorm (n1, n0)\n",
        "        W2 : startwaarde tweede gewichtenmatrix, van de vorm (n2, n1)\n",
        "        b1 : startwaarde eerste biasvector, van de vorm (n1, 1)\n",
        "        b2 : startwaarde tweede biasvector, van de vorm (n2, 1)\n",
        "        num_epochs : hoeveel keer gaan we over alle data\n",
        "        mini_batch_size : hoeveel voorbeelden in één mini-batch ?\n",
        "        alpha : stapgrootte van de updata\n",
        "        \n",
        "        returns:  W1, W2, b1, b2, costs  waarbij costs de historiek van de waarde van de kostfunctie voorstelt\n",
        "    \"\"\"\n",
        "    (n, m) = X.shape\n",
        "    num_batches = m // mini_batch_size + (0 if m % mini_batch_size == 0 else 1)\n",
        "    costs = np.zeros(num_epochs)\n",
        "    for epoch in range(num_epochs):\n",
        "        cost_epoch = 0\n",
        "        for batch in range(num_batches):\n",
        "            ## Bepaal een mini-batch\n",
        "            ## START CODE HIER (4 regels)\n",
        "           \n",
        "            ## EINDE CODE HIER\n",
        "            ## Doe voorwaartse propagatie\n",
        "            ## START CODE HIER (1 regel)\n",
        "            \n",
        "            ## EINDE CODE HIER \n",
        "            ## Doe achterwaartse propagatie\n",
        "            ## START CODE HIER (1 regel)\n",
        "            ## EINDE CODE HIER\n",
        "            if False: ## zet op True op de gradiënt te debuggen --> Traag !\n",
        "                dW1_check, dW2_check, db1_check, db2_check = gradient_check(Xbatch, Ybatch, W1, W2, b1, b2)\n",
        "                assert np.linalg.norm(dW1 - dW1_check) < 10**-6, f\"Foute gradient voor W1 {dW1} {dW1_check}\"\n",
        "                assert np.linalg.norm(dW2 - dW2_check) < 10**-6, \"Foute gradient voor W2\"\n",
        "                assert np.linalg.norm(db1 - db1_check) < 10**-6, \"Foute gradient voor b1\"\n",
        "                assert np.linalg.norm(db2 - db2_check) < 10**-6, \"Foute gradient voor b2\"\n",
        "            ## Voer een update uit\n",
        "            ## START CODE HIER (1 regel)\n",
        "            ## EINDE CODE HIER\n",
        "            # Bepaal kost           \n",
        "            cost_epoch += (batch_end - batch_start + 1) * cost(A2,  Ybatch)\n",
        "        cost_epoch /= m\n",
        "        costs[epoch] = cost_epoch\n",
        "    return W1, W2, b1, b2, costs            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OtlEse-Gfsoc"
      },
      "outputs": [],
      "source": [
        "## Testen op Iris Dataset\n",
        "\n",
        "We testen ons neuraal netwerk op de klassieke \"Iris\" dataset.  Deze bevat gegevens over 150 irissen. Per iris worden er vier attributen gegeven (die te maken hebben met de afmetingen van de iris). Het doel is om te bepalen tot welk van 3 klassen een bepaalde iris behoort.\n",
        "\n",
        "We zullen hiertoe een klein neuraal netwerk opbouwen en trainen m.b.v. de code die we hierboven hebben geïmplementeerd."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "upgUasdyfsoc"
      },
      "outputs": [],
      "source": [
        "We laden de dataset in het geheugen en voeren enkele bewerkingen uit zodanig dat de data heeft die wordt verwacht door het neurale netwerk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "iris = datasets.load_iris()\n",
        "print(f\"Het type van iris.data is {type(iris.data)}\")\n",
        "print(f\"Deze numpy array heeft de volgende vorm: {iris.data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VTNaTGRAfsoj"
      },
      "outputs": [],
      "source": [
        "Het lijkt er m.a.w. op dat <code>iris.data</code> een observatie bevat per rij.  \n",
        "Ons neuraal netwerk verwacht echter één observatie per kolom, i.e. de vorm van <code>X</code> is\n",
        "<code>(n, m)</code>.\n",
        "\n",
        "Schrijf de code om <code>iris.data</code> te transponeren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## BEGIN CODE HIER (1 regel)\n",
        "## EINDE CODE HIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST Cel: Niet wijzigen\n",
        "assert X.shape == (4, 150), \"Verkeerde vorm voor X\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1meMIb-gfsop"
      },
      "outputs": [],
      "source": [
        "Nu bekijken we de labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Het type van iris.target is {type(iris.target)}\")\n",
        "print(f\"De vorm van de numpy array is {iris.target.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wYglUdN0fsos"
      },
      "outputs": [],
      "source": [
        "De verwachte uitvoer <code>Y</code> moet van de vorm <code>(3,150)</code> zijn. Er zijn immers 3 klassen en 150 voorbeelden. We gebruiken een <code>OneHotEncoder</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OneHotEncoder(categories='auto', sparse=False)\n",
        "Y = encoder.fit_transform(iris.target.reshape(-1,1))\n",
        "print(f\"Y.shape = {Y.shape}\")\n",
        "print(\"De eerste 5 rijen van Y zijn\")\n",
        "print(Y[:5,:])\n",
        "print(\"De laatste 5 rijen van Y zijn\")\n",
        "print(Y[-5:,:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tOm9joC6fsov"
      },
      "outputs": [],
      "source": [
        "De array <code>Y/<code> heeft nog steeds de verkeerde vorm. Transponeer <code>Y</code>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## BEGIN CODE HIER (1 regel)\n",
        "## EINDE CODE HIER\n",
        "assert Y.shape == (3,150), \"Verkeerde vorm Y\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_YRwuQfsfso0"
      },
      "outputs": [],
      "source": [
        "De datapunten zijn in dit geval zodanig geordend dat eerst alle voorbeelden van klasse 1 aan bod komen, dan alle voorbeelden van klasse 2 en tenslotte al de voorbeelden van klasse 3.  We permuteren de voorbeelden zodanig dat ze in een random volgorde voorkomen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "permute = np.random.permutation(150)\n",
        "X = X[:,permute]\n",
        "Y = Y[:,permute]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FtURsSvvfso3"
      },
      "outputs": [],
      "source": [
        "We zetten een dertigtal voorbeelden apart om te kunnen nagaan of ook 'nieuwe' voorbeelden goed worden geclassificeerd."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test = X[:,:120], X[:,120:]\n",
        "Y_train, Y_test = Y[:,:120], Y[:,120:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r-4x1oz3fso4"
      },
      "outputs": [],
      "source": [
        "We bouwen nu een klein neuraal netwerk om dit classificatieprobleem aan te pakken.\n",
        "Kies je parameters zodanig dat de verborgen laag 3 neuronen heeft.\n",
        "\n",
        "We gebruiken bv. 1000 epochs met een mini batch grootte van 12."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time \n",
        "## BEGIN CODE HIER\n",
        "\n",
        "## EINDE CODE HIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZMmsMLqfso8"
      },
      "outputs": [],
      "source": [
        "We plotten de kost. Als alles goed gaat dan zie je een sterk dalende curve bij de start van het trainingsproces. Rond de 500 epochs vlakt de kostfunctie af."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(cost_hist)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WmtniREljM8c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uf7msH3Lfso_"
      },
      "outputs": [],
      "source": [
        "Bereken de kost op de trainingsdata. Bepaal hiervoor eerst de activaties voor de trainingsdata en roep vervolgens de kostfunctie aan.  Doe dan hetzelfde voor de testdata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "A1, A2 = forward_propagation(X_train, W1, W2, b1, b2)\n",
        "print(f\"Kost op trainingsdata = {cost(A2, Y_train)}\")\n",
        "A1_test, A2_test = forward_propagation(X_test, W1, W2, b1, b2)\n",
        "print(f\"Kost op testdata = {cost(A2_test, Y_test)}\")      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yeN0Q1mZfspE"
      },
      "outputs": [],
      "source": [
        "Het uiteindelijke doel is classificatie. We bekijken welk percentage van de voorbeelden er juist is geclassificeerd. We kiezen de uitvoerneuron met de hoogste waarde als uiteindelijke klasse."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bepaal voor elke kolom van A2 de positie van het maximum\n",
        "preds = np.argmax(A2, axis = 0)\n",
        "actuals_train = np.argmax(Y_train, axis = 0)\n",
        "print(f\"Het percentage juiste geclassificeerde trainingsvoorbeelden {np.mean(preds == actuals_train)}\")\n",
        "preds_test = np.argmax(A2_test, axis = 0)\n",
        "actuals_test = np.argmax(Y_test, axis = 0)\n",
        "print(f\"Het percentage juiste geclassificeerde testvoorbeelden {np.mean(preds_test == actuals_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o7EHFqcXfspG"
      },
      "outputs": [],
      "source": [
        "We zien dat we hier een zeer goed accuraatheid bekomen zowel op de trainings- als testdata."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QNVub6KggMY8"
      },
      "outputs": [],
      "source": [
        "## MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "foB_oq2XgnNx"
      },
      "outputs": [],
      "source": [
        "We laden de dataset in en bekijken de vorm van de verschillende arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "print('X_train.shape = ', X_train.shape)\n",
        "print('Y_train.shape = ', Y_train.shape)\n",
        "print('X_test.shape = ', X_test.shape)\n",
        "print('Y_test.shape = ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JvM-AqQTg5RK"
      },
      "outputs": [],
      "source": [
        "We bekijken enkele voorbeelden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "    plt.subplot(3, 3, i+1)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
        "    plt.title(\"Class {}\".format(Y_train[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-OgBioQ9hA-7"
      },
      "outputs": [],
      "source": [
        "Er zijn maw 60000 training voorbeelden. Elk voorbeeld is een 28 bij 28 'matrix'. Die zullen we echter gebruiken als een ééndimensionale vector met 784 entries. De labels y zijn opgeslaan als getallen, en niet als een one-hot geëncodeerde vector.\n",
        "\n",
        "Er zijn 10000 voorbeelden apart gehouden om te testen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_flat = X_train.reshape(60000, 28 * 28)\n",
        "X_test_flat = X_test.reshape(10000, 28 * 28)\n",
        "print('Nieuwe shape =', X_train_flat.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tw4zf9T1hUiK"
      },
      "outputs": [],
      "source": [
        "Aangezien we nu toch al keras gebruiken gebruiken we een keras functie om de one-hot encodering uit te voeren.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras.utils\n",
        "Y_train_one_hot = keras.utils.to_categorical(Y_train)\n",
        "Y_test_one_hot = keras.utils.to_categorical(Y_test)\n",
        "print('Nieuwe shape =', Y_train_one_hot.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-p_EEZhmiqI1"
      },
      "outputs": [],
      "source": [
        "De matrices zijn nog altijd niet in de juiste vorm voor ons netwerk. Transponeer ze.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "### BEGIN CODE HIER (4 regels)\n",
        "\n",
        "### EINDE CODE HIER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s5sxhZtxcdL5"
      },
      "outputs": [],
      "source": [
        "Het netwerk zal wellicht sneller kunnen getraind worden als we ervoor zorgen dat alle inputs tussen 0 en 1 liggen.\n",
        "\n",
        "We herschalen de $X$-waarden door deze te delen door 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "### BEGIN CODE HIER\n",
        "\n",
        "### EINDE CODE HIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST CEL: niet wijzigen\n",
        "assert X_train_flat.shape == (28 * 28, 60000), \"Verkeerde vorm X_train_flat\"\n",
        "assert X_test_flat.shape == (28 * 28, 10000), \"Verkeerde vorm X_test_flat\"\n",
        "assert Y_train_one_hot.shape == (10, 60000), \"Verkeerde vorm Y_train_one_hot\"\n",
        "assert Y_test_one_hot.shape == (10, 10000), \"Verkeerde vorm Y_test_one_hot\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ubltevs2hp4s"
      },
      "outputs": [],
      "source": [
        "We trachten nu ons netwerk te trainen. Bouw een netwerk op met 100 neuronen in de verborgen laag.  Train het netwerk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time \n",
        "\n",
        "## BEGIN CODE HIER\n",
        "\n",
        "## EINDE CODE HIER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(cost_hist)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "source": [
        "## Accuraatheid\n",
        "A1, A2 = forward_propagation(X_train_flat, W1, W2, b1, b2)\n",
        "A1_test, A2_test = forward_propagation(X_test_flat, W1, W2, b1, b2)\n",
        "preds = np.argmax(A2, axis = 0)\n",
        "actuals_train = np.argmax(Y_train_one_hot, axis = 0)\n",
        "print(f\"Het percentage juiste geclassificeerde trainingsvoorbeelden {np.mean(preds == actuals_train)}\")\n",
        "preds_test = np.argmax(A2_test, axis = 0)\n",
        "actuals_test = np.argmax(Y_test_one_hot, axis = 0)\n",
        "print(f\"Het percentage juiste geclassificeerde testvoorbeelden {np.mean(preds_test == actuals_test)}\")"
      ]
    }
  ]
}